import os

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from transformers import AutoModelForSequenceClassification, AutoTokenizer

from utils.llm.vectordb import get_vector_db
from utils.llm.tools.datahub import get_glossary_vector_data, get_query_vector_data
from utils.llm.core import get_embeddings


def load_reranker_model(device: str = "cpu"):
    """í•œêµ­ì–´ reranker ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    local_model_path = os.path.join(os.getcwd(), "ko_reranker_local")

    # ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥
    if os.path.exists(local_model_path) and os.path.isdir(local_model_path):
        print("ğŸ”„ ko-reranker ëª¨ë¸ ë¡œì»¬ì—ì„œ ë¡œë“œ ì¤‘...")
    else:
        print("â¬‡ï¸ ko-reranker ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì¤‘...")
        model = AutoModelForSequenceClassification.from_pretrained(
            "Dongjin-kr/ko-reranker"
        )
        tokenizer = AutoTokenizer.from_pretrained("Dongjin-kr/ko-reranker")
        model.save_pretrained(local_model_path)
        tokenizer.save_pretrained(local_model_path)

    return HuggingFaceCrossEncoder(
        model_name=local_model_path,
        model_kwargs={"device": device},
    )


def get_retriever(retriever_name: str = "ê¸°ë³¸", top_n: int = 5, device: str = "cpu"):
    """ê²€ìƒ‰ê¸° íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        retriever_name: ì‚¬ìš©í•  ê²€ìƒ‰ê¸° ì´ë¦„ ("ê¸°ë³¸", "ì¬ìˆœìœ„", ë“±)
        top_n: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
    """
    print(device)
    retrievers = {
        "ê¸°ë³¸": lambda: get_vector_db().as_retriever(search_kwargs={"k": top_n}),
        "Reranker": lambda: ContextualCompressionRetriever(
            base_compressor=CrossEncoderReranker(
                model=load_reranker_model(device), top_n=top_n
            ),
            base_retriever=get_vector_db().as_retriever(search_kwargs={"k": top_n}),
        ),
    }

    if retriever_name not in retrievers:
        print(
            f"ê²½ê³ : '{retriever_name}' ê²€ìƒ‰ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        retriever_name = "ê¸°ë³¸"

    return retrievers[retriever_name]()


def search_tables(
    query: str, retriever_name: str = "ê¸°ë³¸", top_n: int = 5, device: str = "cpu"
):
    """ì¿¼ë¦¬ì— ë§ëŠ” í…Œì´ë¸” ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if retriever_name == "ê¸°ë³¸":
        db = get_vector_db()
        doc_res = db.similarity_search(query, k=top_n)
    else:
        retriever = get_retriever(
            retriever_name=retriever_name, top_n=top_n, device=device
        )
        doc_res = retriever.invoke(query)

    # ê²°ê³¼ë¥¼ ì‚¬ì „ í˜•íƒœë¡œ ë³€í™˜
    documents_dict = {}
    for doc in doc_res:
        lines = doc.page_content.split("\n")

        # í…Œì´ë¸”ëª… ë° ì„¤ëª… ì¶”ì¶œ
        table_name, table_desc = lines[0].split(": ", 1)

        # ì„¹ì…˜ë³„ë¡œ ì •ë³´ ì¶”ì¶œ (í…Œì´ë¸”/ì»¬ëŸ¼ë§Œ ì‚¬ìš©)
        columns = {}
        current_section = None

        for i, line in enumerate(lines[1:], 1):
            line = line.strip()

            # ì„¹ì…˜ í—¤ë” í™•ì¸
            if line == "Columns:":
                current_section = "columns"
                continue

            # ê° ì„¹ì…˜ì˜ ë‚´ìš© íŒŒì‹±
            if current_section == "columns" and ": " in line:
                col_name, col_desc = line.split(": ", 1)
                columns[col_name.strip()] = col_desc.strip()

        # ë”•ì…”ë„ˆë¦¬ ì €ì¥
        documents_dict[table_name] = {
            "table_description": table_desc.strip(),
            **columns,  # ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
        }

    return documents_dict


def _prepare_vector_data(data_fetcher, text_fields):
    """
    ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    points = data_fetcher()
    embeddings = get_embeddings()

    for point in points:
        payload = point["payload"]
        # í…ìŠ¤íŠ¸ í•„ë“œë“¤ì„ ê²°í•©í•˜ì—¬ ì„ë² ë”© ìƒì„±
        text_to_embed = " ".join([str(payload.get(field, "")) for field in text_fields])
        vector = embeddings.embed_query(text_to_embed)
        point["vector"] = {"dense": vector}

    return points


def search_glossary(query: str, force_update: bool = False, top_n: int = 5) -> list:
    """
    ìš©ì–´ì§‘ ê²€ìƒ‰ í•¨ìˆ˜
    """
    collection_name = "lang2sql_glossary"
    db = get_vector_db()

    # ë°ì´í„° ë¡œë” ì •ì˜ (ì„ë² ë”© ìƒì„± í¬í•¨)
    def data_loader():
        return _prepare_vector_data(get_glossary_vector_data, ["name", "description"])

    # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
    db.initialize_collection_if_empty(
        collection_name=collection_name,
        force_update=force_update,
        data_loader=data_loader,
    )

    # ê²€ìƒ‰ ìˆ˜í–‰
    embeddings = get_embeddings()
    query_vector = embeddings.embed_query(query)

    results = db.search(
        collection_name=collection_name,
        query_vector=("dense", query_vector),
        limit=top_n,
    )

    # ê²°ê³¼ í¬ë§·íŒ…
    return [res.payload for res in results]


def search_query_examples(
    query: str, force_update: bool = False, top_n: int = 5
) -> list:
    """
    ì¿¼ë¦¬ ì˜ˆì œ ê²€ìƒ‰ í•¨ìˆ˜
    """
    collection_name = "lang2sql_query_example"
    db = get_vector_db()

    # ë°ì´í„° ë¡œë” ì •ì˜ (ì„ë² ë”© ìƒì„± í¬í•¨)
    def data_loader():
        return _prepare_vector_data(get_query_vector_data, ["name", "description"])

    # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
    db.initialize_collection_if_empty(
        collection_name=collection_name,
        force_update=force_update,
        data_loader=data_loader,
    )

    # ê²€ìƒ‰ ìˆ˜í–‰
    embeddings = get_embeddings()
    query_vector = embeddings.embed_query(query)

    results = db.search(
        collection_name=collection_name,
        query_vector=("dense", query_vector),
        limit=top_n,
    )

    # ê²°ê³¼ í¬ë§·íŒ…
    return [res.payload for res in results]
